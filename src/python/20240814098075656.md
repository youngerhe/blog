---
title: python爬虫之谷歌搜索
category:
  - python
date: 2024-08-10
star: true
tag:
  - python
  - 爬虫
  - selenium
---

## 一、介绍

### 目前我知道的主流爬虫有两种：

1. 通过 http 请求，抓取页面数据，匹配自己所需要的数据。
2. 通过操作浏览器事件，模拟用户行为，获取页面数据，匹配自己所需要的数据。

## 二、方式 1-selenium 操作浏览器

### 2.1、安装依赖

- 安装 python 包

```
pip3 install selenium
pip3 install openpyxl
```

- 安装谷歌浏览器

### 2.2、代码实现

- 目前实现的逻辑是：从 excel 中读取关键词循环爬，遇到校验重启浏览器（目前没发现跳不过去的校验），爬好的内容导入 excel，仅做学习。

```python
class SearchSelenium:
    def __init__(self, keywords):

        self.keywords_last = None
        self.domains = None
        self.keywords = None
        self.set_domain()
        if keywords is None:
            self.set_keywords()
        else:
            self.keywords = copy.deepcopy(keywords)
            self.keywords_last = copy.deepcopy(keywords)

    def set_domain(self):
        with open("../ip/domain.txt", 'r', encoding='utf-8') as f:
            data = f.readlines()
            data = [i.strip('\n').strip() for i in data]
        self.domains = data
        print("..设置google地址池完成")

    def get_domain(self):
        return random.choice(self.domains)

    def set_keywords(self):
        workbook = openpyxl.load_workbook("keywords/keyword01.xlsx")
        sheetR = workbook.active

        # 读取第一列的数据
        a_column_values = [cell.value for cell in sheetR['A'] if cell.value is not None and cell.value != "关键词"]
        b_column_values = [cell.value for cell in sheetR['B'] if cell.value is not None]
        b_column_values.extend(a_column_values)
        self.keywords = copy.deepcopy(b_column_values)
        self.keywords_last = copy.deepcopy(b_column_values)
        workbook.close()

        print("..设置keywords完成")

    def data_handle(self, options, datas):
        workbook = openpyxl.Workbook()
        sheet = workbook.active
        sheet['A1'] = '关键词'
        sheet['B1'] = '关键词'
        sheet['C1'] = '标题'
        sheet['D1'] = '描述'
        sheet['E1'] = '链接'
        for data in datas:

            title, link, desc = "", "", ""
            try:
                # 首栏异形判断
                irregularDiv = data.find_element(By.CSS_SELECTOR, "div.g.wF4fFd.JnwWd.g-blk")

                try:
                    titleH3 = irregularDiv.find_element(By.CSS_SELECTOR, 'h3.LC20lb')
                    title = titleH3.text
                except NoSuchElementException:
                    print("没有找到irregularDiv titleH3")
                    continue
                try:
                    linkA = irregularDiv.find_element(By.TAG_NAME, 'a')
                    link = linkA.get_attribute("href")
                except NoSuchElementException:
                    print("没有找到irregularDiv linkA")
                try:
                    descSpan = irregularDiv.find_element(By.CSS_SELECTOR, 'span.hgKElc')
                    desc = descSpan.text
                except NoSuchElementException:
                    print("没有找到irregularDiv descSpan")
                sheet.append([options['keyword'], options['keyword'], title, desc, link])
                continue
            except NoSuchElementException:
                try:
                    normalDiv = data.find_element(By.CSS_SELECTOR, 'div.g.Ww4FFb.vt6azd.tF2Cxc.asEBEc')
                    try:
                        titleH3 = normalDiv.find_element(By.CSS_SELECTOR, 'h3.LC20lb')
                        title = titleH3.text
                    except NoSuchElementException:
                        print("没有找到normalDiv titleH3")
                        continue
                    try:
                        linkA = normalDiv.find_element(By.TAG_NAME, 'a')
                        link = linkA.get_attribute("href")
                    except NoSuchElementException:
                        print("没有找到normalDiv linkA")
                    try:
                        descDiv = normalDiv.find_element(By.CSS_SELECTOR, 'div.VwiC3b')
                        desc = descDiv.text
                        pattern = r'\d{4}年\d{1,2}月\d{1,2}日 — '
                        desc = re.sub(pattern, '', desc).replace("\xa0", " ")
                    except NoSuchElementException:
                        print("没有找到normalDiv descDiv")
                    sheet.append([options['keyword'], options['keyword'], title, desc, link])
                except NoSuchElementException:
                    dataHtml = data.get_attribute("outerHTML")
                    print("没有找到normalDiv, 88吧", dataHtml)
        workbook.save(f"data/{options['keyword']}.xlsx")
        workbook.close()

    def run(self):
        driver = webdriver.Chrome()
        # TODO 如果是热词是不允许查50条的，判断小于50 就加一个&start=1
        driver.get(f"https://www.google.com/search?q=test&num=50")

        for keyword in self.keywords:
            options = {
                'keyword': keyword,
                'domain': self.get_domain(),
            }
            try:
                search_box = driver.find_element(By.NAME, "q")
                search_box.clear()
                search_box.send_keys(keyword)
                search_box.send_keys(Keys.RETURN)
                # driver.get(f"https://www.google.com/search?q=test&num=100")
                try:
                    datas = WebDriverWait(driver, 10).until(
                        EC.presence_of_all_elements_located((By.CLASS_NAME, "MjjYud")))
                    self.data_handle(options, datas)
                except NoSuchElementException:
                    print(f"失败，keyword:{options['keyword']}，domain:{options['domain']}")
            except:
                print(f"send 出错了。{keyword},{len(self.keywords_last)},{len(self.keywords)}")
                driver.close()
                keywords = self.keywords_last
                keywords.insert(0, keyword)
                dl = SearchSelenium(keywords)
                dl.run()
                continue
            self.keywords_last.remove(keyword)
            print(f"当前关键词：{keyword},关键词数量：{len(self.keywords_last)},{len(self.keywords)}")
        pprint(self.keywords)
        pprint(self.keywords_last)
        driver.close()


if __name__ == "__main__":90
    dl = SearchSelenium(None)
    dl.run()

```

### 三、方式 2-Http 请求

### 3.1、安装依赖

```shell
pip3 install requests
pip3 install bs4
pip3 install openpyxl
```

3.2、代码实现

实现和方式一大同小异，这个方式如果没有代理池基本就凉在人机校验上了。

```python
import random
import openpyxl
import requests
import re
from time import sleep
from bs4 import BeautifulSoup
from fake_useragent import UserAgent

# get方式的爬虫，没有代理池基本凉凉。
class search:
    def __init__(self):
        self.domains = None
        self.proxies_list = None
        self.ua = None
        self.keywords = None

        self.set_proxies()
        self.set_user_agent()
        self.set_keywords()
        self.set_domain()

    def set_domain(self):
        with open("../ip/domain.txt", 'r', encoding='utf-8') as f:
            data = f.readlines()
            data = [i.strip('\n').strip() for i in data]
        self.domains = data
        print("..设置google地址池完成")


    def set_proxies(self):
        proxies_file = open("../ip/IP.txt", "r")
        proxies_list = proxies_file.readlines()
        data = []
        for item in proxies_list:
            v = eval(item.replace('\n', ''))  # 以换行符分割，转换为dict对象
            data.append(v)
        self.proxies_list = data
        proxies_file.close()
        print("")
        print("..设置代理池完成")

    def get_domain(self):
        return random.choice(self.domains)

    def set_user_agent(self):
        self.ua = UserAgent()
        print("..设置UA池完成")

    def set_keywords(self):
        workbookR = openpyxl.load_workbook("keywords/关键词1.xlsx")
        sheetR = workbookR.active

        # 读取第一列的数据
        a_column_values = [cell.value for cell in sheetR['A'] if cell.value is not None and cell.value != "关键词"]
        b_column_values = [cell.value for cell in sheetR['B'] if cell.value is not None]
        b_column_values.extend(a_column_values)
        self.keywords = b_column_values
        workbookR.close()
        print("..设置keywords完成")

    def get_proxies(self):
        return random.choice(self.proxies_list)

    def get_header(self):
        return {
            'User-Agent': self.ua.random,
        }

    def data_handle(self, options, data):
        workbook = openpyxl.Workbook()
        sheet = workbook.active
        sheet['A1'] = '关键词'
        sheet['B1'] = '关键词'
        sheet['C1'] = '标题'
        sheet['D1'] = '描述'
        sheet['E1'] = '链接'
        soup = BeautifulSoup(data.content, "html.parser")
        findByDivClassList = soup.find_all('div', class_='MjjYud')

        # 循环所有结果
        for divClass in findByDivClassList:

            # 首栏异形判断
            irregularDiv = divClass.find('div', class_='g wF4fFd JnwWd g-blk')
            title, link, desc = "", "", ""
            if irregularDiv is not None:
                titleH3 = irregularDiv.find('h3', class_='LC20lb')
                if titleH3 is not None:
                    title = titleH3.text
                linkA = irregularDiv.find('a')
                if linkA is not None:
                    link = linkA.get('href')
                descSpan = irregularDiv.find('span', class_='hgKElc')
                if descSpan is not None:
                    desc = descSpan.text
                sheet.append([options['keyword'], options['keyword'], title, desc, link])
                continue
            normalDiv = divClass.find('div', class_='g Ww4FFb vt6azd tF2Cxc asEBEc')
            # 正常格式
            if normalDiv is not None:
                titleH3 = normalDiv.find('h3', class_='LC20lb')
                if titleH3 is not None:
                    title = titleH3.text
                linkA = normalDiv.find('a')
                if linkA is not None:
                    link = linkA.get("href")
                descDiv = normalDiv.find('div', class_='VwiC3b')
                if descDiv is not None:
                    desc = descDiv.text
                pattern = r'\d{4}年\d{1,2}月\d{1,2}日 — '
                desc = re.sub(pattern, '', desc).replace("\xa0", " ")
                sheet.append([options['keyword'], options['keyword'], title, desc, link])
                continue
        workbook.save(f"data/{options['keyword']}.xlsx")
        workbook.close()

    def run(self):
        for keyword in self.keywords:
            options = {
                'keyword': keyword,
                'proxy': self.get_proxies(),
                'header': self.get_header(),
                'domain': self.get_domain(),
            }
            search_url = f"https://{options['domain']}/search?q={options['keyword']}&num=100"
            session = requests.Session()
            resp = session.get(search_url, headers=options['header'], proxies={'http': options['proxy']['http']})
            session.close()
            if resp.status_code == 200:
                self.data_handle(options, resp)
                print(
                    f"成功，keyword:{options['keyword']}，domain:{options['domain']}，proxy:{options['proxy']['http']}，header:{options['header']}", )
            else:
                print(
                    f"失败，keyword:{options['keyword']}，domain:{options['domain']}，proxy:{options['proxy']['http']}，header:{options['header']},错误码：{resp.status_code}")

            sleep(4)


if __name__ == "__main__":
    dl = search()
    dl.run()

```

### 四、代理池（没啥用）

```python
import re

import requests
from lxml import etree
import time
import json

# 代理池爬取，没啥用，爬出来免费的用不了
class daili:

    def send_request(self, page):
        print("=============正在抓取第{}页===========".format(page))
        base_url = 'https://www.kuaidaili.com/free/inha/{}/'.format(page)
        headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/95.0.4638.69 Safari/537.36'
	}
        response = requests.get(base_url, headers=headers)
        data = response.content.decode()
        time.sleep(1)

        return data

    # 2.解析数据
    def parse_data(self, data):
        pattern = r'const fpsList = (.*?)]'
        match = re.search(pattern, data, re.DOTALL)
        fps_list = []
        if match:
            # 从匹配结果中提取fpsList的内容（作为字符串）
            fps_list_str = match.group(1)

            # 将字符串转换为JSON对象，再转换为Python列表
            fps_list = json.loads(fps_list_str + "]")

        return fps_list

    # 4.检测代理IP
    def check_ip(self, proxies_list):
        headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/95.0.4638.69 Safari/537.36'}

        can_use = []
        for proxies in proxies_list:
            try:
                response = requests.get('https://www.google.com/', headers=headers, proxies={'http': proxies['http']}, timeout=1)
                if response.status_code == 200:
                    can_use.append(proxies)

            except Exception as e:
                print(e)

        return can_use

    # 5.保存到文件
    def save(self, can_use):

        file = open('IP.txt', 'w')
        for i in range(len(can_use)):
            s = str(can_use[i]) + '\n'
            file.write(s)
        file.close()

    # 实现主要逻辑
    def run(self):
        proxies_list = []
        # 实现翻页
        for page in range(1, 20):
            data = self.send_request(page)
            parse_list = self.parse_data(data)
            # 3.获取数据
            for parse_data in parse_list:
                proxies_dict = {}
                proxies_dict["http"] = "http://"+parse_data['ip'] + ":" + parse_data['port']
                proxies_list.append(proxies_dict)
        print("获取到的代理IP数量：", len(proxies_list))
        can_use = self.check_ip(proxies_list)
        print("能用的代理IP数量：", len(can_use))
        print("能用的代理IP:", can_use)
        self.save(can_use)

if __name__ == "__main__":
    dl = daili()
    dl.run()

```
